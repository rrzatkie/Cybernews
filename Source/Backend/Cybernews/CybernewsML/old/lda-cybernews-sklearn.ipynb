{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction import text as text1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LOGS_ENABLED = True\n",
    "\n",
    "#region Functions\n",
    "\n",
    "def log_to_console(logs):\n",
    "    if(LOGS_ENABLED):\n",
    "        pprint(logs)\n",
    "\n",
    "\n",
    "def basic_clean(df):\n",
    "    # Convert to list\n",
    "    data = df['CONTENT'].tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub(\"\\\\S*@\\\\S*\\\\s?\", '', doc) for doc in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub(\"\\\\s+\", ' ', doc) for doc in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\\\'\", \"\", doc) for doc in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    results = list(sent_to_words(data))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def build_dw_matrix(data):\n",
    "    f = open(\"stop_words\", 'r')\n",
    "    words = f.readline().split(\",\")\n",
    "    stop_words= []\n",
    "\n",
    "    for word in words:\n",
    "        tmp = word.strip()\n",
    "        stop_words.append(tmp)\n",
    "\n",
    "    my_stop_words = text1.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        min_df=10,                        # minimum reqd occurences of a word \n",
    "        stop_words=my_stop_words,             # remove stop words\n",
    "        lowercase=True,                   # convert all words to lowercase\n",
    "        token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "        # max_features=50000,             # max number of uniq words\n",
    "    )\n",
    "\n",
    "    return vectorizer, vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "def build_lda_model(data):\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_topics=20,               # Number of topics\n",
    "        max_iter=10,               # Max learning iterations\n",
    "        learning_method='online',\n",
    "        random_state=100,          # Random state\n",
    "        batch_size=128,            # n docs in each learning iter\n",
    "        evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "        n_jobs = -1,               # Use all available CPUs\n",
    "    )\n",
    "    result_matrix = lda_model.fit_transform(data)\n",
    "\n",
    "    log_to_console(lda_model)  # Model attributes\n",
    "\n",
    "    return lda_model, result_matrix\n",
    "\n",
    "\n",
    "def diagnose_model(model, data):\n",
    "    # Log Likelyhood: Higher the better\n",
    "    log_to_console(\"Log Likelihood: {}\".format(model.score(data)))\n",
    "\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    log_to_console(\"Perplexity: {}\".format(model.perplexity(data)))\n",
    "\n",
    "    # See model parameters\n",
    "    log_to_console(model.get_params())\n",
    "\n",
    "\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "# df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df = pd.read_json('./entries.json', lines=True)\n",
    "# log_to_console(df.target_names.unique())\n",
    "\n",
    "\n",
    "values = df.values.tolist()[0]\n",
    "columns = ['CONTENT']\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(len(values)):\n",
    "    cell = []\n",
    "    value = values[i]\n",
    "    cell = [value]\n",
    "    data.append(cell)\n",
    "new_df = pd.DataFrame(data, columns=columns)\n",
    "df = new_df\n",
    "\n",
    "\n",
    "lang_model_name = 'en_core_web_md'\n",
    "\n",
    "nlp = spacy.load(lang_model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_words = basic_clean(df)\n",
    "# data_tokenized = tokenize(data_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['CONTENT']\n",
    "docs = []\n",
    "\n",
    "for text in texts:\n",
    "    docs.append(nlp(text))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = []\n",
    "\n",
    "for doc in docs:\n",
    "    lemmas = []\n",
    "    lemmas_new =[]\n",
    "\n",
    "    lemmas = ([word.lemma_ if word.lemma_ !='-PRON-' else '' for word in doc if word.pos_=='NOUN'])\n",
    "\n",
    "    for token in lemmas:\n",
    "        if(token not in lemmas_new):\n",
    "            lemmas_new.append(token)\n",
    "    \n",
    "    data_words.append(lemmas_new)\n",
    "    \n",
    "    lemmas = \" \".join([token for token in lemmas_new])\n",
    "\n",
    "    data_lemmatized.append(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatization(nlp, texts, allowed_postags):\n",
    "    \n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent))\n",
    "#         texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "#     return texts_out\n",
    "\n",
    "\n",
    "# # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "# data_lemmatized = lemmatization(nlp, data_tokenized, allowed_postags=['NOUN'])\n",
    "\n",
    "# Create the Document-Word matrix\n",
    "vectorizer, data_vectorized = build_dw_matrix(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Model\n",
    "lda_model = LatentDirichletAllocation()\n",
    "lda_model.learning_decay=0.9\n",
    "lda_model.n_components=10\n",
    "\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lth = data_vectorized.shape[0]\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(data_vectorized[i,:].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_vectorized[0,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 11\n",
    "n_top_words = 15\n",
    "\n",
    "tfidf = data_vectorized\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
