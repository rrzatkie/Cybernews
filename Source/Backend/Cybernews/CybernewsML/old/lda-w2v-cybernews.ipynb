{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link to tutorial: https://www.kaggle.com/vukglisovic/classification-combining-lda-and-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# languange processing imports\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = pd.read_csv('./entries-cyware-api-2-cleaned.csv', index_col=0)\n",
    "f = open('entries-lemmatized.csv', 'r')\n",
    "train_data = pd.read_csv(f, index_col=0, converters={'text_lemmatized': lambda x: x[1:-1].replace(\"'\", \"\").split(', ')})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=raw_data[raw_data['isnull'] == False]\n",
    "train_data['num_of_words']=list(map(len, train_data.text.str.split(' ')))\n",
    "train_data=train_data[(train_data.num_of_words > 100) & (train_data.num_of_words < 3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = np.array(list(map(len, train_data.text.str.split(' '))))\n",
    "\n",
    "print(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\n",
    "print(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\n",
    "print(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.set_title(\"Distribution of number of words\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of words\")\n",
    "sns.distplot(document_lengths, bins=50, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "our_special_word = 'qwerty'\n",
    "\n",
    "\n",
    "def containsNonAscii(word):\n",
    "    return any(ord(i)>127 for i in word)\n",
    "\n",
    "def remove_nonascii_words(text):\n",
    "    words = \"\".join(text.split('.')).split(' ')\n",
    "    cleaned_words = [word for word in words if  not containsNonAscii(word)]\n",
    "#     pbar.update(1)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "inputs = train_data.text.values\n",
    "# pbar = tqdm(total=len(inputs))\n",
    "rows = Parallel(n_jobs=2)(delayed(remove_nonascii_words)(i) for i in inputs)\n",
    "train_data['text_nonascii_removed'] = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append(our_special_word)\n",
    "    return [word for word in doc if word not in stopwords]\n",
    "\n",
    "def tokenize(doc):\n",
    "    doc = nltk.word_tokenize(doc)\n",
    "    return remove_stopwords(doc)\n",
    "\n",
    "count = len(train_data.text_nonascii_removed.values)\n",
    "texts_tokenized = []\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "i=0\n",
    "incr = 40\n",
    "\n",
    "texts_part = train_data.text_nonascii_removed.values[i:i+incr]\n",
    "\n",
    "with tqdm(total=count) as pbar:\n",
    "    while(len(texts_tokenized) < count):\n",
    "        inputs = texts_part\n",
    "        rows = Parallel(n_jobs=num_cores)(delayed(tokenize)(i) for i in inputs)\n",
    "        texts_tokenized.extend(rows)\n",
    "        \n",
    "        i += incr\n",
    "        texts_part = train_data.text_nonascii_removed.values[i:i+incr]\n",
    "        pbar.update(incr)\n",
    "        \n",
    "train_data['text_tokenized'] = texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, string\n",
    "lang_model_name = 'en'\n",
    "nlp = spacy.load(lang_model_name, disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def lemmatize(text_tokenized):\n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "    results = []\n",
    "    docs = nlp.pipe([\" \".join(x) for x in text_tokenized])\n",
    "    for doc in docs:\n",
    "        lemmas = []\n",
    "\n",
    "        lemmas = [word.lemma_ if word.lemma_ not in['-PRON-', 'qwerty'] else '' \n",
    "                               for word in doc \n",
    "                               if(('caption' not in word.lemma_) &\n",
    "                                      (word.pos_ in ['NOUN', 'PROPN']) &\n",
    "                                      (word.is_punct == False))]\n",
    "        print(lemmas)\n",
    "        result = []\n",
    "        for token in lemmas:\n",
    "            if((token not in result) & (token not in punctuations) & (len(token) >= 3)):\n",
    "                result.append(token)\n",
    "        results.append(result) \n",
    "    return results\n",
    "    \n",
    "    \n",
    "count = len(train_data.text_tokenized.values)\n",
    "texts_lemmatized = []\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "i=0\n",
    "incr = 32\n",
    "\n",
    "texts_parts = [train_data.text_tokenized.values[i:i+incr], train_data.text_tokenized.values[i+incr:i+(2*incr)]]\n",
    "\n",
    "with tqdm(total=count) as pbar:\n",
    "    pbar.update(i)\n",
    "    while(len(texts_lemmatized) < (count)):\n",
    "        inputs = texts_parts\n",
    "        rows = Parallel(n_jobs=num_cores)(delayed(lemmatize)(i) for i in inputs)\n",
    "        \n",
    "        for rows_part in rows:\n",
    "            texts_lemmatized.extend(rows_part)\n",
    "        \n",
    "        i += (2*incr)\n",
    "        texts_parts = [train_data.text_tokenized.values[i:i+incr], train_data.text_tokenized.values[i+incr:i+(2*incr)] ]\n",
    "        pbar.update(incr*len(rows))    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('entries-lemmatized.csv', 'r')\n",
    "train_data_lemmatized = pd.read_csv(f, index_col=0, converters={'text_lemmatized': lambda x: x[1:-1].replace(\"'\", \"\").split(', ')})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.text_tokenized.values[-1])\n",
    "print(train_data.text_lemmatized.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('entries-lemmatized.csv', \"w\")\n",
    "train_data.to_csv(path_or_buf=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(train_data.tokenized_text.values, min_count=5, threshold=100)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "train_data['bigrams']= make_bigrams(train_data.stopwords_removed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(documents=train_data.text_lemmatized.values)\n",
    "\n",
    "print(\"Found {} words.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_above=0.8, no_below=3)\n",
    "\n",
    "dictionary.compactify()  # Reindexes the remaining words after filtering\n",
    "print(\"Left with {} words.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_bow(df):\n",
    "    df['bow'] = list(map(lambda doc: dictionary.doc2bow(doc), df.text_lemmatized))\n",
    "    \n",
    "document_to_bow(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_words_df = pd.DataFrame.from_dict(dictionary.token2id, orient='index')\n",
    "cleansed_words_df.rename(columns={0: 'id'}, inplace=True)\n",
    "\n",
    "cleansed_words_df['count'] = list(map(lambda id_: dictionary.dfs.get(id_), cleansed_words_df.id))\n",
    "del cleansed_words_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_words_df.sort_values('count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = word_frequency_barplot(cleansed_words_df)\n",
    "ax.set_title(\"Document Frequencies (Number of documents a word appears in)\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_data.bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 10\n",
    "#A multicore approach to decrease training time\n",
    "LDAmodel = LdaMulticore(corpus=corpus,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=num_topics,\n",
    "                        workers=4,\n",
    "                        chunksize=4000,\n",
    "                        passes=7,\n",
    "                        alpha='asymmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_lda_features(lda_model, document):\n",
    "    \"\"\" Transforms a bag of words document to features.\n",
    "        It returns the proportion of how much each topic was\n",
    "        present in the document.\n",
    "    \"\"\"\n",
    "    topic_importances = LDAmodel.get_document_topics(document, minimum_probability=0)\n",
    "    topic_importances = np.array(topic_importances)\n",
    "    return topic_importances[:,1]\n",
    "\n",
    "train_data['lda_features'] = list(map(lambda doc: document_to_lda_features(LDAmodel, doc), train_data.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LDAmodel.get_document_topics(train_data.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_top_words(lda_model, topic_id, nr_top_words=15):\n",
    "    \"\"\" Returns the top words for topic_id from lda_model.\n",
    "    \"\"\"\n",
    "    id_tuples = lda_model.get_topic_terms(topic_id, topn=nr_top_words)\n",
    "    word_ids = np.array(id_tuples)[:,0]\n",
    "    words = map(lambda id_: lda_model.id2word[id_], word_ids)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = train_data['lda_features'].mean()\n",
    "\n",
    "for x in sorted(np.argsort(distribution)[-15:]):\n",
    "    top_words = get_topic_top_words(LDAmodel, x)\n",
    "    print(\"\\nFor topic {} :\\n {}.\".format(x, \",\\n \".join(top_words)))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 1000\n",
    "n_topics = 37\n",
    "n_top_words = 11\n",
    "\n",
    "\n",
    "data = train_data.text_lemmatized.values\n",
    "new_data = []\n",
    "\n",
    "for doc in data:\n",
    "    new_data.append(\" \".join(doc))\n",
    "\n",
    "data = new_data\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        min_df=200,                        # minimum reqd occurences of a word\n",
    "        lowercase=True,                   # convert all words to lowercase\n",
    "        token_pattern='[a-zA-Z0-9]{4,}',  # num chars >= 4\n",
    "        max_features=50000,             # max number of uniq words\n",
    "    )\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(data_vectorized)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "\n",
    "# raw_data = pd.read_csv('./entries-cyware-api-2-cleaned.csv', index_col=0)\n",
    "f = open('entries-lemmatized.csv', 'r')\n",
    "train_data = pd.read_csv(f, index_col=0, converters={'text_lemmatized': lambda x: x[1:-1].replace(\"'\", \"\").split(', ')})\n",
    "f.close()\n",
    "\n",
    "train_data = train_data.reindex(np.random.permutation(train_data.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([20, 1, 9, 34, 12, 18, 7, 30, 26, 14, 13, 11, 32, 23, 16, 33], dtype='int64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "categories = train_data.category\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(categories)\n",
    "categories_dict = { idx:c for idx, c in enumerate(encoder.classes_)}\n",
    "\n",
    "categories_counts = train_data.categories_encoded.value_counts()\n",
    "categories_counts_filtered_indexes = categories_counts[categories_counts > 1000].index\n",
    "categories_counts_filtered_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data.loc[train_data['categories_encoded'].isin(categories_counts_filtered_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_count = 1000\n",
    "train_set = pd.DataFrame()\n",
    "\n",
    "for c in categories_counts_filtered_indexes:\n",
    "    temp = train_data.loc[train_data.categories_encoded == c][:set_count]\n",
    "    train_set = train_set.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(['text_tokenized', 'category', 'categories_encoded'], axis=1).text_lemmatized.values\n",
    "Y = train_set.categories_encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder = LabelEncoder()\n",
    "new_categories = Y\n",
    "\n",
    "new_encoder.fit(new_categories)\n",
    "new_categories_dict = { c:idx for idx, c in enumerate(new_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, c in enumerate(new_categories):\n",
    "    new_categories[idx] = new_categories_dict[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = new_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        max_features=4000\n",
    "    )\n",
    "\n",
    "new_X = []\n",
    "\n",
    "for doc in X:\n",
    "    new_X.append(\" \".join(doc))\n",
    "\n",
    "X = new_X\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(len(numpy.unique(numpy.array(Y_test))))\n",
    "# # print(numpy.unique(numpy.array(Y_train)))\n",
    "\n",
    "# print(len(numpy.bincount(numpy.array(Y_test))))\n",
    "# # Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "# # Y_test = tf.keras.utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.439375\n"
     ]
    }
   ],
   "source": [
    "# SKLEARN CLASSIFIER\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "svclassifier = OneVsOneClassifier(LinearSVC(random_state=0), n_jobs=-1)\n",
    "svclassifier.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "Y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#KERAS CLASIIFIER\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "\n",
    "nof_categories = len(new_encoder.classes_)\n",
    "nof_features = X_train.shape[1]\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Dense(, activation='relu', input_dim=nof_features),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(nof_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = SGD(lr=0.0001, decay=2e-6, momentum=0.9, nesterov=False)\n",
    "adam = Adam(lr=0.00005, beta_1=0.2, beta_2=0.999, epsilon=None, decay=2e-6, amsgrad=False)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=80, batch_size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, batch_size=1)\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_train, Y_train, batch_size=32)\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.callbacks.History.on_batch_begin(model, model.batch()\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].nonzero()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do sprawdzenia bardziej recall i precision - zamiast accuracy\n",
    "# overfitting\n",
    "# SVM \n",
    "# croissanty i świeżo wyciskany sok dla Madzi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
