{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"entries.json\"\n",
    "SYNC_REQUIRED = True\n",
    "CALC_SIMILARITY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['art_id', 'text', 'tags']\n",
    "entries = pd.DataFrame(columns=columns)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries_json_file = Path(FILE_NAME)\n",
    "\n",
    "if entries_json_file.is_file():\n",
    "    resp = requests.get(\"https://cybernews.rrzatkie.xyz/api/entries/?offset={}\".format(i)) \n",
    "    count = int(json.loads(resp.text)[\"count\"])\n",
    "    \n",
    "    entries = pd.read_json(entries_json_file)\n",
    "    cur_count = len(entries.values)\n",
    "    \n",
    "    SYNC_REQUIRED = (cur_count != count)\n",
    "\n",
    "if(SYNC_REQUIRED):\n",
    "    entries = pd.DataFrame(columns=columns)\n",
    "    resp = requests.get(\"https://cybernews.rrzatkie.xyz/api/entries/?offset={}\".format(i)) \n",
    "    json_content = json.loads(resp.text)\n",
    "\n",
    "    content= json_content[\"results\"]\n",
    "    count = int(json_content[\"count\"])\n",
    "    \n",
    "    with tqdm(total=count) as pbar:\n",
    "        for entry in content:\n",
    "            df_temp = pd.DataFrame([[\"id-{}\".format(entry['id']), entry[\"article\"][\"text\"],entry[\"tags\"]]], columns=columns)\n",
    "            entries = entries.append(df_temp, ignore_index=True)\n",
    "\n",
    "        i += len(content)\n",
    "        pbar.update(len(content))\n",
    "        \n",
    "        while(i < count):\n",
    "            resp = requests.get(\"https://cybernews.rrzatkie.xyz/api/entries/?offset={}\".format(i)) \n",
    "            resp_content = json.loads(resp.text)[\"results\"]\n",
    "\n",
    "            for entry in resp_content:\n",
    "                df_temp = pd.DataFrame([[\"id-{}\".format(entry['id']), entry[\"article\"][\"text\"],entry[\"tags\"]]], columns=columns)\n",
    "                entries = entries.append(df_temp, ignore_index=True)\n",
    "\n",
    "            i += len(resp_content)\n",
    "            pbar.update(len(content))\n",
    "\n",
    "    print(\"Expected: {}, got: {}\".format(count, len(entries.values)))\n",
    "\n",
    "\n",
    "    f = open(FILE_NAME, \"w\")\n",
    "    f.write(entries.to_json())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries['tags_count'] = entries.tags.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_with_tags = entries[entries.tags_count != 0]\n",
    "entries_without_tags = entries[entries.tags_count == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries_with_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries_without_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = requests.get(\"https://cybernews.rrzatkie.xyz/api/tags\")\n",
    "tags = [tag['name'] for tag in json.loads(tags.text)['results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('svc_model_pipeline.pickle', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lsvc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc_model_pipeline.pickle', 'rb') as f:\n",
    "    model_lsvc = pickle.load(f)\n",
    "    print('model loaded OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_lsvc.predict([entries_with_tags.text.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = [[tags[idx] for idx, topic in enumerate(doc) if topic == 1] for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tags = [tags[i] for i in results.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tags = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tags:\n",
    "    distinct_tags[tag] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags in entries_with_tags.tags.values:\n",
    "    for tag in tags:\n",
    "        distinct_tags[tag]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_with_tags.tags_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(distinct_tags.keys())\n",
    "names.append('NONE')\n",
    "values = list(distinct_tags.values())\n",
    "values.append(len(entries_without_tags.tags.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(names,values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = model_lsvc.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lsvc.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_with_tags['lemmatized_text'] = preprocessor.transform(entries_with_tags['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = model_lsvc.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_vectorizer = CountVectorizer(\n",
    "    analyzer='word',       \n",
    "    min_df=10,                        # minimum reqd occurences of a word \n",
    "    stop_words='english',             # remove stop words\n",
    "    lowercase=True,                   # convert all words to lowercase\n",
    "    token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "    # max_features=50000             ,# max number of uniq words\n",
    ")\n",
    "\n",
    "entries_with_tags['lemmatized_text'] = preprocessor.inverse_transform(entries_with_tags['lemmatized_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = cv_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(entries_with_tags['lemmatized_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Init the Model\n",
    "lda_model = LatentDirichletAllocation()\n",
    "lda_model.learning_decay=0.9\n",
    "lda_model.n_components=10\n",
    "\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.stem('achieve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
