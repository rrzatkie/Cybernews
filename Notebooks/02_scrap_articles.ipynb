{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import newspaper as nwp\n",
    "\n",
    "f = open('raw_data_cyware-api-14-08-2019.csv', 'r', encoding=\"utf-8\")\n",
    "df = pd.read_csv(f, index_col=0)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_and_analyze(df_row):\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        url = df_row['web_sp_link']\n",
    "        article = nwp.Article(url)\n",
    "        article.download()\n",
    "        \n",
    "        # Wait for article to be downloaded (max 2s)\n",
    "        max_time=0.0\n",
    "        while((article.download_state == 0) & (max_time < 0.5)):\n",
    "            time.sleep(0.1)\n",
    "            max_time += 0.1\n",
    "\n",
    "        # Parse article and scrap the text content\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "    except nwp.ArticleException:\n",
    "        print(\"Article failed to download.\\nUrl:{}\\nTime: {}\".format(url, str(datetime.datetime.now())))\n",
    "    finally:\n",
    "        df_row['text_scraped']=text\n",
    "        return df_row \n",
    "\n",
    "scraped_articles = []\n",
    "count = len(df.values)\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "with tqdm(total=count) as pbar:\n",
    "    while(count != 0):\n",
    "        inputs = df[count-num_cores:count].to_dict('records')\n",
    "\n",
    "        rows = Parallel(n_jobs=num_cores)(delayed(fetch_and_analyze)(i) for i in inputs)\n",
    "        scraped_articles.extend(rows)\n",
    "        \n",
    "        pbar.update(len(rows))\n",
    "        count -= len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from newspaper import news_pool\n",
    "\n",
    "def fetch_and_analyze(df_row):\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        url = df_row['web_sp_link']\n",
    "        article = nwp.Article(url)\n",
    "        article.download()\n",
    "        \n",
    "        # Wait for article to be downloaded (max 2s)\n",
    "        max_time=0.0\n",
    "        while((article.download_state == 0) & (max_time < 0.5)):\n",
    "            time.sleep(0.1)\n",
    "            max_time += 0.1\n",
    "\n",
    "        # Parse article and scrap the text content\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "    except nwp.ArticleException:\n",
    "        print(\"Article failed to download.\\nUrl:{}\\nTime: {}\".format(url, str(datetime.datetime.now())))\n",
    "    finally:\n",
    "        df_row['text_scraped']=text\n",
    "        return df_row \n",
    "\n",
    "articles = df[0:2].to_dict('records')\n",
    "scraped_articles = []\n",
    "papers = []\n",
    "\n",
    "for article in articles:\n",
    "    papers.append(nwp.build(article['url']))\n",
    "\n",
    "news_pool.set(papers, threads_per_source=1)\n",
    "news_pool.join()\n",
    "\n",
    "for i in range(len(papers)):\n",
    "    articles[i]['text_scraped'] = papers[i].articles[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import news_pool\n",
    "\n",
    "url1 = df.iloc[1]['web_sp_link']\n",
    "\n",
    "paper1 = nwp.build(url1)\n",
    "\n",
    "papers = [paper1]\n",
    "\n",
    "news_pool.set(papers, threads_per_source=2)\n",
    "news_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hotel Data Breach - Largest lodging franchisors Choice Hotels has recently disclosed that it has suffered a data breach that occurred due to a misconfigured MongoDB database. Click here to know more!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper1.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cyware.com/news/misconfigured-mongodb-database-leaks-700000-choice-hotels-customer-records-8894a252'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<newspaper.source.Category at 0x26b11c0fc18>,\n",
       " <newspaper.source.Category at 0x26b12f7d908>,\n",
       " <newspaper.source.Category at 0x26b14f40048>,\n",
       " <newspaper.source.Category at 0x26b14f402b0>,\n",
       " <newspaper.source.Category at 0x26b14f402e8>,\n",
       " <newspaper.source.Category at 0x26b14f40278>,\n",
       " <newspaper.source.Category at 0x26b14f40f28>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
